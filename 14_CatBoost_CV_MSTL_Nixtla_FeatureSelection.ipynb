{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25880ecf-fac9-4940-b534-f8ef19dcc290",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.ensemble._hist_gradient_boosting._gradient_boosting'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdarts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeSeries\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdarts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FFT, ExponentialSmoothing\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcb\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdt\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\models\\__init__.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdarts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mforecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkalman_forecaster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KalmanForecaster\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdarts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mforecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_regression_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegressionModel\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdarts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mforecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom_forest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForest\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdarts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mforecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression_ensemble_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegressionEnsembleModel\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdarts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mforecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegressionModel\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\darts\\models\\forecasting\\random_forest.py:20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mRandom Forest\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m-------------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m.. [1] https://en.wikipedia.org/wiki/Random_forest\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdarts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_logger\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdarts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mforecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     FUTURE_LAGS_TYPE,\n\u001b[0;32m     25\u001b[0m     LAGS_TYPE,\n\u001b[0;32m     26\u001b[0m     RegressionModel,\n\u001b[0;32m     27\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\__init__.py:15\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_forest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     ExtraTreesClassifier,\n\u001b[0;32m      9\u001b[0m     ExtraTreesRegressor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     RandomTreesEmbedding,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_gb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradientBoostingClassifier, GradientBoostingRegressor\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hist_gradient_boosting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgradient_boosting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     HistGradientBoostingClassifier,\n\u001b[0;32m     17\u001b[0m     HistGradientBoostingRegressor,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_iforest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IsolationForest\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stacking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StackingClassifier, StackingRegressor\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:49\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     41\u001b[0m     _check_monotonic_cst,\n\u001b[0;32m     42\u001b[0m     _check_sample_weight,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m     check_is_fitted,\n\u001b[0;32m     48\u001b[0m )\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_gradient_boosting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _update_raw_predictions\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbinning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _BinMapper\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m G_H_DTYPE, X_DTYPE, Y_DTYPE\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.ensemble._hist_gradient_boosting._gradient_boosting'"
     ]
    }
   ],
   "source": [
    "## Get libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, QuantileRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import pickle\n",
    "import mlflow\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "from darts import TimeSeries\n",
    "from darts.models import FFT, ExponentialSmoothing\n",
    "import catboost as cb\n",
    "import datetime as dt\n",
    "import math\n",
    "from src.seasonal import MultiSeasonalDecomposition # STL, FourierDecomposition,\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import MSTL, AutoARIMA, AutoETS, Naive\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose, STL\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93de796-5bf9-4753-983f-50c6219c20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33208500-8fda-4e73-ad2a-78f02056bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "exp_no = 13\n",
    "exp_desc = 'Catboost model + Mul Seas + MSTL for month'\n",
    "\n",
    "target_col = 'Temperature'\n",
    "tracking_col = [\"id\", 'date']\n",
    "\n",
    "windows_cv_num_periods = 6 # Split the data into 10 parts and perform windows based TS cross validation and \n",
    "                            # Out of sample TS-prediction. (These TS-predictions will go as input to final model)\n",
    "evaluation_start_date = '2018-04-04'\n",
    "time_feature_origin = '2016-01-01 00:00:00'\n",
    "\n",
    "# Coverage - Represents prediction interval expected coverage in %.\n",
    "quantiles = [i * 0.10 for i in range(1, 10)] # probabilities for which we should predict.\n",
    "quantiles.insert(0, 0.05)\n",
    "quantiles = [round(q, 3) for q in quantiles]\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f3968-3fa1-4113-9f93-5a1db7893bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_quantiles = []\n",
    "for quantile in quantiles:\n",
    "    \n",
    "    lb = round(quantile/2, 3)\n",
    "    ub = 1 - lb\n",
    "    lb = str(lb)\n",
    "    ub = str(ub) #'%.3f' % ub\n",
    "    new_quantiles.append(lb)\n",
    "    new_quantiles.append(ub)\n",
    "\n",
    "new_quantiles.insert(0, '0.5')\n",
    "new_quantiles.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aacfbb-e458-4f74-aa0e-802fb78da95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0833d0e-bf5a-4241-80cb-e364fbf885e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get data \n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "sample_df = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d0024-4a0b-4694-9169-0cb7f01380c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cyclic encoding \n",
    "\n",
    "def sin_cos_encoding(column, column_max = None):\n",
    "    \"\"\"\n",
    "    Encode a pandas DataFrame column into sine and cosine values.\n",
    "\n",
    "    Parameters:\n",
    "    - column: pandas Series\n",
    "        The input column to be encoded.\n",
    "\n",
    "    Returns:\n",
    "    - sin_values: list\n",
    "        List of sine values for each element in the input column.\n",
    "    - cos_values: list\n",
    "        List of cosine values for each element in the input column.\n",
    "\n",
    "    This function takes a pandas DataFrame column, computes the sine and cosine\n",
    "    values for each element in the column, and returns two lists containing\n",
    "    these encoded values. The encoding is useful for converting cyclic data,\n",
    "    such as angles or time, into a format that can be more easily processed\n",
    "    by machine learning models.\n",
    "    \"\"\"\n",
    "    if column_max is None:\n",
    "        max_value = column.max()\n",
    "    else:\n",
    "        max_value = column_max\n",
    "    \n",
    "    sin_values = [math.sin((2*math.pi*x)/max_value) for x in list(column)]\n",
    "    cos_values = [math.cos((2*math.pi*x)/max_value) for x in list(column)]\n",
    "    return sin_values, cos_values\n",
    "\n",
    "\n",
    "# Define the function to transform and rename columns\n",
    "def cyclic_feature_transformation(df, columns, column_max = None):\n",
    "\n",
    "    for index, column_name in enumerate(columns):\n",
    "        \n",
    "        if column_max is None:\n",
    "            i_column_max = None\n",
    "        else:\n",
    "            i_column_max = column_max[index]\n",
    "            \n",
    "        # Call cyclic_feature_transformation function\n",
    "        sin_values, cos_values = sin_cos_encoding(df[column_name], i_column_max)\n",
    "        \n",
    "        # Rename the columns\n",
    "        new_column_name_sin = f'{column_name}_sin'\n",
    "        new_column_name_cos = f'{column_name}_cos'\n",
    "        # df.rename(columns={column_name: new_column_name_sin, \n",
    "        #                    new_column_name_sin: new_column_name_cos}, inplace=True)\n",
    "        \n",
    "        # Assign new values to the renamed columns\n",
    "        df[new_column_name_sin] = sin_values\n",
    "        df[new_column_name_cos] = cos_values\n",
    "        \n",
    "        # Drop the original column\n",
    "        df.drop(column_name, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebe176-90de-4450-9ebc-c6143cce0cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_obj = dt.datetime.strptime(time_feature_origin,  '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# add time dependant variable.\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "# train_df['new_date'] = pd.to_datetime(train_df['date'])\n",
    "# train_df['day_of_week'] = train_df.date.dt.dayofweek\n",
    "train_df['day_of_year'] = train_df.date.dt.day_of_year\n",
    "train_df['hour'] = train_df.date.dt.hour\n",
    "train_df['day_of_month'] = train_df.date.dt.day\n",
    "# train_df['week_of_year'] = train_df.date.dt.isocalendar().week\n",
    "# train_df['month'] = train_df.date.dt.month\n",
    "# train_df['quarter'] = train_df.date.dt.quarter\n",
    "# train_df['year'] = train_df.date.dt.year\n",
    "# train_df.set_index(\"new_date\", inplace=True)\n",
    "# train_df['date_int'] = (train_df['date'] - dt_obj) / np.timedelta64(1, 'h')\n",
    "\n",
    "# cyclic_feature_transformation(train_df, ['day_of_week', 'day_of_year', 'month'], [7, 366, 12])\n",
    "cyclic_feature_transformation(train_df, ['hour', 'day_of_year', 'day_of_month'], [24, 366, 31])\n",
    "\n",
    "test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "# test_df['new_date'] = pd.to_datetime(test_df['date'])\n",
    "# test_df['day_of_week'] = test_df.date.dt.dayofweek\n",
    "test_df['day_of_year'] = test_df.date.dt.day_of_year\n",
    "test_df['hour'] = test_df.date.dt.hour\n",
    "test_df['day_of_month'] = test_df.date.dt.day\n",
    "# test_df['week_of_year'] = test_df.date.dt.isocalendar().week\n",
    "# test_df['month'] = test_df.date.dt.month\n",
    "# test_df['quarter'] = test_df.date.dt.quarter\n",
    "# test_df['year'] = test_df.date.dt.year\n",
    "# test_df.set_index(\"new_date\", inplace=True)\n",
    "# test_df['date_int'] = (test_df['date'] - dt_obj) / np.timedelta64(1, 'h')\n",
    "\n",
    "# cyclic_feature_transformation(test_df, ['day_of_week', 'day_of_year', 'month'], [7, 366, 12])\n",
    "cyclic_feature_transformation(test_df, ['hour', 'day_of_year', 'day_of_month'], [24, 366, 31])\n",
    "\n",
    "train_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f868e8d7-da5f-439b-b77d-ebc6cd9518ff",
   "metadata": {},
   "source": [
    "### Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ce617-191b-4c29-ab90-22355d42deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Carl McBride's functions.\n",
    "\n",
    "def crps(submission, solution):\n",
    "    \"\"\"\n",
    "    This routine returns the mean continuous ranked probability score (CRPS).\n",
    "    Each individual CRPS score is numerically integrated using 23 points.\n",
    "    The extremal points (100% coverage) are competition fixed at -30 and 60.\n",
    "    The \"submission\" dataframe: the last 21 columns should be the predictions\n",
    "    The \"solution\" dataframe must contain a \"Temperature\" column (the \"ground truth\")\n",
    "    \n",
    "    Author: Carl McBride Ellis\n",
    "    Version: 1.0.0\n",
    "    Date: 2024-03-30\n",
    "    \"\"\"\n",
    "        \n",
    "    # A list of the requested quantile values, along with added 100% coverage endpoints \n",
    "    # (these values are all competition fixed)\n",
    "    # the 0.5 quantile is the \"zero coverage\" forecast i.e. the median point prediction\n",
    "    quantiles = [0.00, 0.025, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 0.975, 1.00]\n",
    "    submission_tmp = submission.copy()\n",
    "    # inset the y_true values to the submission_tmp dataframe to the LHS\n",
    "    submission_tmp.insert(0, \"Temperature\", solution[\"Temperature\"].values)\n",
    "    \n",
    "    CRPS = 0\n",
    "    for index, row in submission_tmp.iterrows():\n",
    "        x_values = row[-(len(quantiles)-2):] # column name agnostic\n",
    "        y_true = row[\"Temperature\"] # the ground truth value\n",
    "        \n",
    "        x_values = [float(i) for i in x_values] # make sure all x values are floats\n",
    "        # add extremal 100% quantile x-values so as to be sure to bracket all possible y_true values\n",
    "        # note: any changing of these values will change the score\n",
    "        x_values.append(-30.0)\n",
    "        x_values.append( 60.0)\n",
    "        x_values.sort() # sort x values into ascending order (no quantile crossing)\n",
    "\n",
    "        # split predictions to the left and right of the true value\n",
    "        # get items below the true value (y_true)\n",
    "        LHS_keys = [i for i,x in enumerate(x_values) if x < y_true]\n",
    "        # get items above the true value (y_true)\n",
    "        RHS_keys = [i for i,x in enumerate(x_values) if x >= y_true]\n",
    "\n",
    "        # quantiles and predictions below the true value (y_true)\n",
    "        LHS_values = [x_values[i] for i in LHS_keys]\n",
    "        LHS_quantiles = [quantiles[i] for i in LHS_keys]\n",
    "\n",
    "        # quantiles and predictions above the true value (y_true)\n",
    "        RHS_values = [x_values[i] for i in RHS_keys]\n",
    "        RHS_quantiles = [quantiles[i] for i in RHS_keys]\n",
    "\n",
    "        # also calculate quantile at y (q_at_y_true)\n",
    "        x1, y1 = LHS_values[-1], LHS_quantiles[-1]\n",
    "        x2, y2 = RHS_values[0], RHS_quantiles[0]\n",
    "        q_at_y_true = ((y2-y1)*(y_true-x1)/(x2-x1))+y1\n",
    "\n",
    "        # add y_true and q_at_y_true to RHS of LHS list\n",
    "        LHS_values.append(y_true)\n",
    "        LHS_quantiles.append(q_at_y_true)\n",
    "\n",
    "        # add y_true and q_at_y_true to LHS of RHS list\n",
    "        RHS_values.insert(0, y_true)\n",
    "        RHS_quantiles.insert(0, q_at_y_true)\n",
    "\n",
    "        # integrate the LHS as a sum of trapezium for CDF**2\n",
    "        LHS_integral = 0\n",
    "        for i in range(len(LHS_values)-1):\n",
    "            LHS_integral += (0.5 * (LHS_values[i+1]-LHS_values[i]) * (LHS_quantiles[i]**2 + LHS_quantiles[i+1]**2) )\n",
    "\n",
    "        # integrate the RHS as a sum of trapezium for (1-CDF)**2\n",
    "        RHS_integral = 0\n",
    "        for i in range(len(RHS_values)-1):\n",
    "            RHS_integral += (0.5 * (RHS_values[i+1]-RHS_values[i]) * ((1-RHS_quantiles[i])**2 +(1-RHS_quantiles[i+1])**2 ) )\n",
    "\n",
    "        CRPS += (LHS_integral + RHS_integral)\n",
    "\n",
    "    del submission_tmp\n",
    "    # calculate the mean CRPS\n",
    "    CRPS = CRPS/len(submission)\n",
    "    return CRPS\n",
    "\n",
    "\n",
    "def coverage_report(submission, solution):\n",
    "    \"\"\"\n",
    "    Version: 1.0.1\n",
    "    \"\"\"\n",
    "    y_true = solution[\"Temperature\"].values\n",
    "    # this does not take the \"zero coverage\" prediction into account\n",
    "    # which is assumed to be located in submission.csv column -11\n",
    "    coverages = [95, 90, 80, 70, 60, 50, 40, 30, 20, 10]\n",
    "    N = len(coverages)\n",
    "    # ANSI color codes\n",
    "    BOLD_RED = '\\033[1;31m'\n",
    "    BOLD_GREEN = '\\033[1;32m'\n",
    "    END_COLOR = '\\033[0m'\n",
    "    \n",
    "    def mean_coverage(y_pred_low,y_true,y_pred_up):\n",
    "        return ( (y_pred_low <= y_true) & (y_pred_up >= y_true) ).mean()\n",
    "    \n",
    "    for i, coverage in enumerate(coverages):\n",
    "        lower_col, upper_col = (2*N+1-i), (i+1)\n",
    "        actual_coverage = mean_coverage(submission.iloc[:,-lower_col], y_true, submission.iloc[:,-upper_col])\n",
    "        actual_coverage = round(actual_coverage*100,2)\n",
    "        if actual_coverage >= coverages[i]:\n",
    "            print(BOLD_GREEN, \"Ideal: {}% Actual: {}% [PASS]\".format(coverage, actual_coverage), END_COLOR)\n",
    "        else:\n",
    "            print(BOLD_RED, \"Ideal: {}% Actual: {}% [FAIL]\".format(coverage, actual_coverage), END_COLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613fa15a-3455-40f6-9803-fc9fa2b2d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertDataFrameToArrays(train_df, test_df, target_col, tracking_col):\n",
    "    ## Convert data to model consumable format.\n",
    "\n",
    "    # Extract features (X) and target variable (y) from the DataFrame    \n",
    "    cols_to_rem = [target_col] + tracking_col\n",
    "    X_train = train_df.drop(columns=cols_to_rem)  \n",
    "    y_train = train_df[target_col]\n",
    "    X_test = test_df.drop(columns = tracking_col)  \n",
    "\n",
    "    # Convert DataFrame to numpy arrays\n",
    "    X_train_array = X_train.values\n",
    "    y_train_array = y_train.values\n",
    "    X_test_array = X_test.values\n",
    "\n",
    "    array_dict = {\n",
    "        'X_Train': X_train_array,\n",
    "        'y_Train': y_train_array,\n",
    "        'X_Test': X_test_array\n",
    "                 }\n",
    "    return(array_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08f1a4-3898-4fbb-83b0-7ca9c1370fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Capture_Monthly_Seasonal_Effect(i_train_df):\n",
    "    # i_train_df.loc[:, 'Seasonality_month'] = res_new.seasonal['month'].to_list()\n",
    "    df = i_train_df[['date', 'Residual']]\n",
    "    df.rename(columns = {'date':'ds', 'Residual':'y'}, inplace=True)\n",
    "    df.insert(1, \"unique_id\", 1)\n",
    "    \n",
    "    models = [MSTL(\n",
    "        season_length=[2922], # seasonalities of the time series 1) hourly * 4 2) monthly 365.25/12*24*4\n",
    "        trend_forecaster=AutoARIMA() # model used to forecast trend\n",
    "    )]\n",
    "    \n",
    "    sf = StatsForecast(\n",
    "        models=models, # model used to fit each time series \n",
    "        freq='15min', # frequency of the data\n",
    "    )\n",
    "\n",
    "    sf = sf.fit(df=df)\n",
    "\n",
    "    decomposition_df = sf.fitted_[0, 0].model_\n",
    "    # print(decomposition_df)\n",
    "    df.loc[:, 'MSTL_Trend'] = decomposition_df.loc[:, 'trend']\n",
    "    df.loc[:, 'Monthly_Seasonality'] = decomposition_df.loc[:, 'seasonal']\n",
    "\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665a206-de51-41ef-a15a-c3d9f0900a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSTL_TS(i_train_df, i_validation_df, mstl_nr_freqs_to_keep):\n",
    "\n",
    "    fmt = '%Y-%m-%d %H:%M:%S'\n",
    "    d_max = dt.datetime.strptime(str(max(i_train_df['date'])), fmt)\n",
    "    d_min = dt.datetime.strptime(str(min(i_train_df['date'])), fmt)\n",
    "    \n",
    "    week_diff = (d_max - d_min).days / 7\n",
    "\n",
    "    if(week_diff > 52):\n",
    "        mstl_seasonality_periods=[\"day_of_year\", \"month\", \"hour\"] # , \"day_of_week\"\n",
    "    else:\n",
    "        mstl_seasonality_periods=[\"month\", \"hour\"]\n",
    "    \n",
    "    i_ts_train_df = i_train_df.loc[:, [\"date\", target_col]]\n",
    "    i_ts_train_df['date'] = pd.to_datetime(i_ts_train_df['date'])\n",
    "    i_ts_train_df = i_ts_train_df.set_index(\"date\")\n",
    "    # i_ts_train = TimeSeries.from_series(i_ts_train_df)\n",
    "    i_ts_train = i_ts_train_df[target_col]\n",
    "    \n",
    "    # MSTL decomposition of training data \n",
    "    stl = MultiSeasonalDecomposition(seasonal_model=\"fourier\",\n",
    "                                     seasonality_periods=mstl_seasonality_periods, \n",
    "                                     model = \"additive\", \n",
    "                                     n_fourier_terms=mstl_nr_freqs_to_keep)\n",
    "    res_new = stl.fit(pd.Series(i_ts_train, index=i_ts_train_df.index))\n",
    "    \n",
    "    # adding decomposed components to df\n",
    "    if(week_diff > 52):\n",
    "        i_train_df.loc[:, 'Seasonality_Day_of_Year'] = res_new.seasonal['day_of_year'].to_list()\n",
    "    i_train_df.loc[:, 'Seasonality_hour'] = res_new.seasonal['hour'].to_list()\n",
    "    i_train_df.loc[:, 'Trend'] = res_new.trend.to_list()\n",
    "    i_train_df.loc[:, 'Residual'] = res_new.resid.to_list()\n",
    "\n",
    "    # the monthly component has not been captured properly. We are doing this using the MSTL model.\n",
    "    df = Capture_Monthly_Seasonal_Effect(i_train_df)\n",
    "    i_train_df.loc[:, 'Trend'] = df['MSTL_Trend'].to_list()\n",
    "    i_train_df.loc[:, 'Seasonality_month'] = df['Monthly_Seasonality'].to_list()\n",
    "    \n",
    "    # Fitting FFT model to seasonal components and forecasting them for the test data.\n",
    "    if(week_diff > 52):  \n",
    "        i_sts_train = i_train_df.loc[:, [\"date\", 'Seasonality_Day_of_Year']].set_index(\"date\")\n",
    "        i_sts_train = TimeSeries.from_series(i_sts_train)\n",
    "        fft_model = FFT(nr_freqs_to_keep=mstl_nr_freqs_to_keep, trend=None)\n",
    "        fft_model.fit(i_sts_train)\n",
    "        i_sts_validation = fft_model.predict(len(i_validation_df['date']))\n",
    "        i_validation_df.loc[:, 'Seasonality_Day_of_Year'] = i_sts_validation.pd_series().to_list()\n",
    "    \n",
    "    i_sts_train = i_train_df.loc[:, [\"date\", 'Seasonality_month']].set_index(\"date\")\n",
    "    i_sts_train = TimeSeries.from_series(i_sts_train)\n",
    "    fft_model = FFT(nr_freqs_to_keep=mstl_nr_freqs_to_keep, trend=None)\n",
    "    fft_model.fit(i_sts_train)\n",
    "    i_sts_validation = fft_model.predict(len(i_validation_df['date']))\n",
    "    i_validation_df.loc[:, 'Seasonality_month'] = i_sts_validation.pd_series().to_list()\n",
    "    \n",
    "    i_sts_train = i_train_df.loc[:, [\"date\", 'Seasonality_hour']].set_index(\"date\")\n",
    "    i_sts_train = TimeSeries.from_series(i_sts_train)\n",
    "    fft_model = FFT(nr_freqs_to_keep=mstl_nr_freqs_to_keep, trend=None)\n",
    "    fft_model.fit(i_sts_train)\n",
    "    i_sts_validation = fft_model.predict(len(i_validation_df['date']))\n",
    "    i_validation_df.loc[:, 'Seasonality_hour'] = i_sts_validation.pd_series().to_list()\n",
    "    \n",
    "    # Exponential smoothing for trend component and forecasting into the future.\n",
    "    i_tts_train = i_train_df.loc[:, [\"date\", 'Trend']].set_index(\"date\")\n",
    "    i_tts_train = TimeSeries.from_series(i_tts_train)\n",
    "    es_model = ExponentialSmoothing(damped=True, \n",
    "                                    seasonal=None)\n",
    "    es_model.fit(i_tts_train)\n",
    "    i_tts_validation = es_model.predict(len(i_validation_df['date']))\n",
    "    i_validation_df.loc[:, 'Trend'] = i_tts_validation.pd_series().to_list()\n",
    "\n",
    "    \n",
    "    if(week_diff > 52):\n",
    "        i_ts_validation = (i_validation_df['Seasonality_Day_of_Year'] + \n",
    "                           i_validation_df['Seasonality_hour'] + \n",
    "                           i_validation_df['Seasonality_month'] + \n",
    "                           i_validation_df['Trend'])\n",
    "        # ideally we should add the residual, but having some artificial error seems to help later on.\n",
    "        i_fit = (i_train_df['Seasonality_Day_of_Year'] + \n",
    "                 i_train_df['Seasonality_hour'] + \n",
    "                 i_train_df['Seasonality_month'] + \n",
    "                 i_train_df['Trend'] - \n",
    "                 i_train_df['Residual'])\n",
    "        # i_train_df.drop(columns=['Seasonality_Day_of_Year', 'Seasonality_hour', 'Seasonality_month', 'Trend', 'Residual'], inplace=True)\n",
    "        # i_validation_df.drop(columns=['Seasonality_Day_of_Year', 'Seasonality_hour', 'Seasonality_month', 'Trend'], inplace=True)\n",
    "    else:\n",
    "        i_ts_validation = (i_validation_df['Seasonality_hour'] + \n",
    "                           i_validation_df['Seasonality_month'] + \n",
    "                           i_validation_df['Trend'])\n",
    "        # ideally we should add the residual, but having some artificial error seems to help later on.\n",
    "        i_fit = (i_train_df['Seasonality_hour'] + \n",
    "                 i_train_df['Seasonality_month'] + \n",
    "                 i_train_df['Trend'] - \n",
    "                 i_train_df['Residual'])\n",
    "        \n",
    "        # i_train_df.drop(columns=['Seasonality_hour', 'Seasonality_month', 'Trend', 'Residual'], inplace=True)\n",
    "        # i_validation_df.drop(columns=['Seasonality_hour', 'Seasonality_month', 'Trend'], inplace=True)\n",
    "    i_train_df.drop(columns=['Residual'], inplace=True)\n",
    "    return(i_train_df, i_validation_df, i_fit, i_ts_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3d693-6bd6-43d2-b9ae-df75804a1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TsPredAsFeature(train_df, test_df, target_col, windows_cv_num_periods, \n",
    "                    mstl_nr_freqs_to_keep = 35, fft_trend = None, fft_trend_poly_degree = 1):\n",
    "    \n",
    "    fmt = '%Y-%m-%d %H:%M:%S'\n",
    "    d_max = dt.datetime.strptime(str(max(train_df['date'])), fmt)\n",
    "    d_min = dt.datetime.strptime(str(min(train_df['date'])), fmt)\n",
    "    \n",
    "    time_diff = (d_max - d_min).total_seconds()\n",
    "    period_time_diff = time_diff/windows_cv_num_periods\n",
    "    \n",
    "    train_period_end = []\n",
    "    validation_period_end = []\n",
    "    train_df.loc[:, 'TS_Pred'] = np.nan\n",
    "\n",
    "    # generating out os sample TS values for training data\n",
    "    for i in list(range(windows_cv_num_periods - 1)):\n",
    "        \n",
    "        train_period_end.append(d_min + dt.timedelta(seconds=period_time_diff * (i + 1)))\n",
    "        validation_period_end.append(d_min + dt.timedelta(seconds=period_time_diff * (i + 2)))\n",
    "    \n",
    "        i_train_df = train_df[train_df['date'] <= train_period_end[i]]\n",
    "        i_validation_df = train_df[((train_df['date'] >  train_period_end[i]) & \n",
    "                                    (train_df['date'] <= validation_period_end[i]))]\n",
    "    \n",
    "        # fft_model = FFT(nr_freqs_to_keep=mstl_nr_freqs_to_keep, trend=fft_trend, trend_poly_degree = fft_trend_poly_degree)\n",
    "        # fft_model.fit(i_ts_train)\n",
    "    \n",
    "        # i_ts_validation = fft_model.predict(len(i_validation_df['date']))\n",
    "        i_train_df, i_validation_df, i_fit, i_ts_validation = MSTL_TS(i_train_df, i_validation_df, mstl_nr_freqs_to_keep)\n",
    "        \n",
    "        if(i == 0):     \n",
    "            mask = (train_df['date'] <= train_period_end[i])  \n",
    "            train_df.loc[mask, 'TS_Pred'] = i_fit.to_list()\n",
    "    \n",
    "        mask = (train_df['date'] > train_period_end[i]) & (train_df['date'] <= validation_period_end[i])\n",
    "        train_df.loc[mask, 'TS_Pred'] = i_ts_validation.to_list()\n",
    "\n",
    "    train_df['TS_Pred'].fillna((train_df[target_col].mean()), inplace=True)\n",
    "\n",
    "    train_df.loc[train_df['TS_Pred'].isnull(), 'TS_Pred'] = train_df[target_col].mean()\n",
    "    \n",
    "    # generating TS values for test data.\n",
    "    train_df, test_df, i_fit, i_ts_validation = MSTL_TS(train_df, test_df, mstl_nr_freqs_to_keep)\n",
    "\n",
    "    test_df.loc[:, 'TS_Pred'] = i_ts_validation.to_list()\n",
    "\n",
    "    out_dict = {\n",
    "        'train_df': train_df,\n",
    "        'test_df' : test_df\n",
    "    }\n",
    "    return(out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184f057-e129-45cd-9b42-95b7b370b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FitModelAndPredict(train_df, X_train_array, y_train_array, test_df, X_test_array, new_quantiles, cb_param):\n",
    "\n",
    "    train_dataset = cb.Pool(X_train_array, y_train_array) \n",
    "\n",
    "    new_quantiles_txt = \",\".join(new_quantiles)\n",
    "    \n",
    "    model = cb.CatBoostRegressor(\n",
    "        loss_function=''.join(['MultiQuantile:alpha=', new_quantiles_txt]), \n",
    "        # loss_function='Quantile:alpha=0.5', # CV on single quantile\n",
    "        logging_level = 'Silent',\n",
    "        random_seed = 42,\n",
    "        depth = cb_param['depth'],\n",
    "        iterations = cb_param['iterations'],\n",
    "        l2_leaf_reg = cb_param['l2_leaf_reg'],\n",
    "        learning_rate = cb_param['learning_rate'])\n",
    "\n",
    "    # model.grid_search(grid, train_dataset)\n",
    "    model.fit(train_dataset)\n",
    "\n",
    "    pred = model.predict(X_train_array)\n",
    "    pred_df = pd.DataFrame(pred, columns=new_quantiles)\n",
    "    pred_df.loc[:, 'id']  = train_df[\"id\"]\n",
    "    new_cols = ['id'] + new_quantiles\n",
    "    train_out = pred_df[new_cols]\n",
    "    \n",
    "    pred = model.predict(X_test_array)\n",
    "    pred_df = pd.DataFrame(pred, columns=new_quantiles)\n",
    "    pred_df.loc[:, 'id']  = test_df[\"id\"]\n",
    "    new_cols = ['id'] + new_quantiles\n",
    "    test_out = pred_df[new_cols]\n",
    "    \n",
    "    out_dict = {\n",
    "        'model': model,\n",
    "        'train_out' : train_out, \n",
    "        'test_out' : test_out\n",
    "    }\n",
    "    return(out_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71879314-6078-4f78-bbe7-7f2053c61245",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28bfdf8-c4d3-4c08-a364-dbddd882dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = train_df[train_df['date'] < evaluation_start_date]\n",
    "test_val_df_t = train_df[train_df['date'] >= evaluation_start_date]\n",
    "test_val_df_t.reset_index(inplace=True, drop = True)\n",
    "test_val_df = test_val_df_t.drop(columns = target_col)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ad6e9-0e3c-4edb-9cc6-8ea02db2a431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find right nr frequencies to keep.\n",
    "# for darts-fft cross validation.\n",
    "df1 = pd.DataFrame({'mstl_nr_freqs_to_keep': [35]}) # 1, 3, 5, 10, 25, 35, 50\n",
    "df2 = pd.DataFrame({'fft_trend_poly_degree': [0], 'fft_trend': [None]})\n",
    "df_ts_iter = df1.merge(df2, how=\"cross\")\n",
    "\n",
    "train_rmse_list = []\n",
    "test_rmse_list = []\n",
    "out_dict_list = []\n",
    "\n",
    "for i in df_ts_iter.index:\n",
    "    mstl_nr_freqs_to_keep = df_ts_iter.loc[i, 'mstl_nr_freqs_to_keep']\n",
    "    fft_trend = df_ts_iter.loc[i, 'fft_trend']\n",
    "    fft_trend_poly_degree = df_ts_iter.loc[i, 'fft_trend_poly_degree']\n",
    "    \n",
    "    out_dict = TsPredAsFeature(train_val_df, test_val_df, target_col, windows_cv_num_periods, \n",
    "                               mstl_nr_freqs_to_keep, fft_trend, fft_trend_poly_degree)\n",
    "    train_val_df = out_dict[\"train_df\"]\n",
    "    test_val_df = out_dict[\"test_df\"]\n",
    "\n",
    "    out_dict_list.append(out_dict)\n",
    "    train_rmse_list.append(root_mean_squared_error(train_val_df[target_col], train_val_df['TS_Pred']))\n",
    "    test_rmse_list.append(root_mean_squared_error(test_val_df_t[target_col], test_val_df['TS_Pred']))\n",
    "\n",
    "    print('-------------------------------------------------')\n",
    "    print(f'mstl_nr_freqs_to_keep: {mstl_nr_freqs_to_keep}')\n",
    "    print(f'fft_trend_poly_degree: {fft_trend_poly_degree}')\n",
    "    print(f'train_rmse: {train_rmse_list[i]}')\n",
    "    print(f'test_rmse: {test_rmse_list[i]}')\n",
    "    print('-------------------------------------------------')\n",
    "\n",
    "df_ts_iter['train_rmse'] = train_rmse_list\n",
    "df_ts_iter['test_rmse'] = test_rmse_list\n",
    "\n",
    "df_ts_iter['weighted_rmse'] = df_ts_iter['train_rmse'] * 0.1 + df_ts_iter['test_rmse'] * 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd805a-b85f-4956-a9bf-3bdb2dce5d1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selected time series hyper parameters\n",
    "rmse_list = df_ts_iter['weighted_rmse'].to_list()\n",
    "ind = rmse_list.index(min(rmse_list))\n",
    "mstl_nr_freqs_to_keep = df_ts_iter.loc[ind, 'mstl_nr_freqs_to_keep']\n",
    "fft_trend_poly_degree = df_ts_iter.loc[ind, 'fft_trend_poly_degree']\n",
    "fft_trend = df_ts_iter.loc[ind, 'fft_trend']\n",
    "out_dict = out_dict_list[ind]\n",
    "train_val_df = out_dict[\"train_df\"]\n",
    "test_val_df = out_dict[\"test_df\"]\n",
    "\n",
    "train_val_df.drop(columns=['TS_Pred'], inplace=True)\n",
    "test_val_df.drop(columns=['TS_Pred'], inplace=True)\n",
    "\n",
    "print(f'index selected: {ind}')\n",
    "print(f'mstl_nr_freqs_to_keep: {mstl_nr_freqs_to_keep}')\n",
    "print(f'fft_trend_poly_degree: {fft_trend_poly_degree}')\n",
    "print(f'fft_trend: {fft_trend}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1b147-d65f-4084-a954-be4e7c1bb14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### df to array\n",
    "array_dict = ConvertDataFrameToArrays(train_val_df, test_val_df, target_col, tracking_col)\n",
    "X_train_val_array = array_dict[\"X_Train\"]\n",
    "y_train_val_array = array_dict[\"y_Train\"]\n",
    "X_test_val_array = array_dict[\"X_Test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596cb692-3462-4e61-9745-98ca4ea98063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame with hyper parameters of all combinations. \n",
    "\n",
    "df1 = pd.DataFrame({'iterations': [100, 200]}) # 150\n",
    "df2 = pd.DataFrame({'learning_rate': [0.03, 0.1]})\n",
    "df3 = pd.DataFrame({'depth': [2, 4]}) # 6, 8\n",
    "df4 = pd.DataFrame({'l2_leaf_reg': [0.2, 3]}) # 0.5, 1\n",
    "\n",
    "df_iter = df1.merge(df2, how=\"cross\")\n",
    "df_iter = df_iter.merge(df3, how=\"cross\")\n",
    "df_iter = df_iter.merge(df4, how=\"cross\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5911766-692e-4564-854e-3397789c6a70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Testing for different hyper parameters.\n",
    "v_rmse_list = []\n",
    "v_crps_list = []\n",
    "out_dict_list = []\n",
    "\n",
    "for i in df_iter.index:\n",
    "\n",
    "    cb_param = {'iterations': df_iter.loc[i, 'iterations'],\n",
    "                'learning_rate': df_iter.loc[i, 'learning_rate'],\n",
    "                'depth': df_iter.loc[i, 'depth'],\n",
    "                'l2_leaf_reg': df_iter.loc[i, 'l2_leaf_reg']}\n",
    "\n",
    "    out_dict = FitModelAndPredict(train_val_df, X_train_val_array, y_train_val_array, test_val_df, X_test_val_array,\n",
    "                                  new_quantiles, cb_param)\n",
    "    \n",
    "    out_dict_list.append(out_dict)\n",
    "    \n",
    "    train_val_out = out_dict[\"train_out\"]\n",
    "    test_val_out = out_dict[\"test_out\"]\n",
    "    model_val = out_dict[\"model\"]\n",
    "\n",
    "    validation_rmse = root_mean_squared_error(test_val_df_t[target_col], test_val_out['0.5'])\n",
    "    validation_crps = crps(test_val_out, test_val_df_t)\n",
    "\n",
    "    v_rmse_list.append(validation_rmse)\n",
    "    v_crps_list.append(validation_crps)\n",
    "    \n",
    "    print('-------------------------------------------------')\n",
    "    print(f'index: {i}')\n",
    "    print(cb_param)\n",
    "    print(f'RMSE: {validation_rmse}')\n",
    "    print(f'CRPS: {validation_crps}')\n",
    "    print('-------------------------------------------------')\n",
    "\n",
    "df_iter['RMSE'] = v_rmse_list\n",
    "df_iter['CRPS'] = v_crps_list\n",
    "df_iter['combined_score'] = df_iter['RMSE'] * 0.5 + df_iter['CRPS'] * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8cce38-5a0c-4c89-bb54-18f753d3da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected cb hyper parameters\n",
    "rmse_list = df_iter['combined_score'].to_list()\n",
    "ind = rmse_list.index(min(rmse_list))\n",
    "\n",
    "cb_param = {'iterations': df_iter.loc[ind, 'iterations'],\n",
    "            'learning_rate': df_iter.loc[ind, 'learning_rate'],\n",
    "            'depth': df_iter.loc[ind, 'depth'],\n",
    "            'l2_leaf_reg': df_iter.loc[ind, 'l2_leaf_reg']}\n",
    "\n",
    "out_dict = out_dict_list[ind]\n",
    "train_val_out = out_dict[\"train_out\"]\n",
    "test_val_out = out_dict[\"test_out\"]\n",
    "model_val = out_dict[\"model\"]\n",
    "\n",
    "print(f'index selected: {ind}')\n",
    "print(f'cb_param: ')\n",
    "print(df_iter.loc[ind, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02367f8d-8478-49c4-9aed-32b8877c8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature selection\n",
    "a\n",
    "v_rmse_list = []\n",
    "v_crps_list = []\n",
    "out_dict_list = []\n",
    "\n",
    "for i in df_iter.index:\n",
    "\n",
    "    \n",
    "\n",
    "    out_dict = FitModelAndPredict(train_val_df, X_train_val_array, y_train_val_array, \n",
    "                                  test_val_df, X_test_val_array,\n",
    "                                  new_quantiles, cb_param)\n",
    "    \n",
    "    out_dict_list.append(out_dict)\n",
    "    \n",
    "    train_val_out = out_dict[\"train_out\"]\n",
    "    test_val_out = out_dict[\"test_out\"]\n",
    "    model_val = out_dict[\"model\"]\n",
    "\n",
    "    validation_rmse = root_mean_squared_error(test_val_df_t[target_col], test_val_out['0.5'])\n",
    "    validation_crps = crps(test_val_out, test_val_df_t)\n",
    "\n",
    "    v_rmse_list.append(validation_rmse)\n",
    "    v_crps_list.append(validation_crps)\n",
    "    \n",
    "    print('-------------------------------------------------')\n",
    "    print(f'index: {i}')\n",
    "    print(cb_param)\n",
    "    print(f'RMSE: {validation_rmse}')\n",
    "    print(f'CRPS: {validation_crps}')\n",
    "    print('-------------------------------------------------')\n",
    "\n",
    "df_iter['RMSE'] = v_rmse_list\n",
    "df_iter['CRPS'] = v_crps_list\n",
    "df_iter['combined_score'] = df_iter['RMSE'] * 0.5 + df_iter['CRPS'] * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0d407-5dc4-4654-949e-c1a723bb7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_rem = [target_col] + tracking_col\n",
    "pd.DataFrame({'feature_importance': model_val.get_feature_importance(), \n",
    "              'feature_names': train_val_df.drop(columns=cols_to_rem).columns}).sort_values(by=['feature_importance'], \n",
    "                                                           ascending=False)\n",
    "\n",
    "# model_val.get_feature_importance()\n",
    "# model_val.feature_names_\n",
    "# train_val_df.drop(columns=cols_to_rem).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9a2bfe-b9f5-402a-923b-a2f78001a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_out.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a166dd-201c-406b-811c-36f8e6004d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df.loc[:, 'pred'] = train_val_out['0.5']\n",
    "train_val_df.loc[:, 'pred_UB'] = train_val_out['0.95']\n",
    "train_val_df.loc[:, 'pred_LB'] = train_val_out['0.05']\n",
    "\n",
    "test_val_df.loc[:, target_col] = test_val_df_t[target_col]\n",
    "test_val_df.loc[:, 'pred'] = test_val_out['0.5']\n",
    "test_val_df.loc[:, 'pred_UB'] = test_val_out['0.95']\n",
    "test_val_df.loc[:, 'pred_LB'] = test_val_out['0.05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeebfabd-4e24-4a5d-860a-b71183816a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot TS vs Actuals train data\n",
    "# melted_df = pd.melt(train_val_df, id_vars=['id', 'date'], \n",
    "#                     value_vars=['Temperature', 'TS_Pred'], \n",
    "#                     var_name='variable', value_name='Temp')\n",
    "# fig = px.line(melted_df, x=\"date\", y=\"Temp\", color = 'variable', title=\"Temperature - Actuals vs TS (FFT) - Train\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d65cee-a0e1-4a28-b292-0e157165fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot TS vs Actuals test data\n",
    "# melted_df = pd.melt(test_val_df, id_vars=['id', 'date'], \n",
    "#                     value_vars=['Temperature', 'TS_Pred'], \n",
    "#                     var_name='variable', value_name='Temp')\n",
    "# fig = px.line(melted_df, x=\"date\", y=\"Temp\", color = 'variable', title=\"Temperature - Actuals vs TS (FFT) - Test\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee0f93-f30e-482a-a8ef-17d8f0f9500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train data\n",
    "melted_df = pd.melt(train_val_df, id_vars=['id', 'date'], \n",
    "                    value_vars=['Temperature', 'pred', 'pred_UB', 'pred_LB'], \n",
    "                    var_name='variable', value_name='Temp')\n",
    "fig = px.line(melted_df, x=\"date\", y=\"Temp\", color = 'variable', title=\"Temperature over time - train\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83289e9-259f-4e6d-a55e-82588045dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot validation data\n",
    "melted_df = pd.melt(test_val_df, id_vars=['id', 'date'], \n",
    "                    value_vars=['Temperature', 'pred', 'pred_UB', 'pred_LB'], \n",
    "                    var_name='variable', value_name='Temp')\n",
    "fig = px.line(melted_df, x=\"date\", y=\"Temp\", color = 'variable', title=\"Temperature over time - test\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1876f58-fc7e-486f-b799-a76a7d1a50ca",
   "metadata": {},
   "source": [
    "### Training using full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c044a59-2376-483f-badd-9fd56ff4ec3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_dict = TsPredAsFeature(train_df, test_df, target_col, windows_cv_num_periods, \n",
    "                           mstl_nr_freqs_to_keep, fft_trend, fft_trend_poly_degree)\n",
    "train_df = out_dict[\"train_df\"]\n",
    "test_df = out_dict[\"test_df\"]\n",
    "\n",
    "train_df.drop(columns=['TS_Pred'], inplace=True)\n",
    "test_df.drop(columns=['TS_Pred'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e5b44-62ea-43da-b090-8a40d4c883c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c7d489-db86-4704-8869-0de273844ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### df to array\n",
    "array_dict = ConvertDataFrameToArrays(train_df, test_df, target_col, tracking_col)\n",
    "\n",
    "# Convert DataFrame to numpy arrays\n",
    "X_train_array = array_dict[\"X_Train\"]\n",
    "y_train_array = array_dict[\"y_Train\"]\n",
    "X_test_array = array_dict[\"X_Test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cdc3b1-012a-4d70-ac5e-eecae7c47822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting model and getting predictions\n",
    "out_dict = FitModelAndPredict(train_df, X_train_array, y_train_array, test_df, X_test_array,\n",
    "                              new_quantiles, cb_param)\n",
    "train_out = out_dict[\"train_out\"]\n",
    "test_out = out_dict[\"test_out\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d04a0b-3049-4ab7-911b-b35fd1536348",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b24ac-9744-4069-8868-ea7e3daf8525",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out.to_csv(f'outputs/test_submission_exp{exp_no}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab703bb-0654-4ef8-b3a5-8c7d0ddfe9bd",
   "metadata": {},
   "source": [
    "### Plotting the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db5613-b2d8-4feb-96f0-2c8c18f48cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "train_df['pred'] = train_out['0.5']\n",
    "train_df['pred_UB'] = train_out['0.95']\n",
    "train_df['pred_LB'] = train_out['0.05']\n",
    "\n",
    "test_df['pred'] = test_out['0.5']\n",
    "test_df['pred_UB'] = test_out['0.95']\n",
    "test_df['pred_LB'] = test_out['0.05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e310b-72e6-424f-9cb1-b63f844dca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sub_df = train_df[['id', 'date', 'Temperature', 'pred', 'pred_UB', 'pred_LB']]\n",
    "test_sub_df = test_df[['id', 'date', 'pred', 'pred_UB', 'pred_LB']]\n",
    "plot_df = pd.concat([train_sub_df, test_sub_df], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2b647-7088-4c4c-a288-acd782ecc119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame\n",
    "melted_df = pd.melt(plot_df, id_vars=['id', 'date'], \n",
    "                    value_vars=['Temperature', 'pred', 'pred_UB', 'pred_LB'], \n",
    "                    var_name='variable', value_name='Temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a508f5-603a-4ffc-9bae-76ed4c96edde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.line(melted_df, x=\"date\", y=\"Temp\", color = 'variable', title=\"Temperature over time\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f75a98-84a4-42e5-8a8d-508834b95f49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
